{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a program to multyply two matrices of size $(100, 100)$ in two methods : (a) by using `np.dot(mat_1, mat_2)` and (b) by using loops. Compare the time of execution in both the cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_1 = np.random.randn(100, 100)\n",
    "mat_2 = np.random.randn(100, 100)\n",
    "out = np.zeros([100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-55.84742721449197\n",
      "Wall time: 883 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        sum = 0\n",
    "        for k in range(100):\n",
    "            sum = sum + mat_1[i, k] * mat_2[k, j]\n",
    "        out[i, j] = sum \n",
    "\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = np.dot(mat_1, mat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-55.84742721449243\n"
     ]
    }
   ],
   "source": [
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. We can clearly see the difference between using for loops and numpy. How fast is numpy from using loops ? <br> Numpy takes : $9.37ms$ <br> and Loop takes : $716ms$ <br> That implies : $\\frac{716}{9.37}=76.414$, Numpy is $76$ times faster than using loops. <br> Numpy uses multiple CPU cores for parallel computation which reduces the running time significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a program to execute the steps below using numpy :\n",
    "  $$z_{ij} = \\sum_{k=1}^{n}w_{ik}x_{kj}$$\n",
    "    $$\\sigma_{ij}(z_{ij}) = \\frac{1}{1+e^{-z_{ij}}}$$ where $\\textbf{w}$ and $\\textbf{x}$ are matrices of random numbers having  dimensions $(m,n)$ and $(n,k)$, respectively, $\\sigma(z)$ is a function which performs above defined operation on elements of $\\textbf{z}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(10, 20)\n",
    "x = np.random.randn(20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(z):\n",
    "    return (1/(1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(w, x)\n",
    "sigma_z = fun(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30)\n",
      "(10, 30)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)\n",
    "print(sigma_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  1.  For performing element wise operation, instead of using loops you can treat a vector/matrix as a normal variable in numpy, and numpy backend will handle the element wise operation very efficiently.\n",
    " 2. The operation that `fun(z)` performs is actually an activation function used in neural netwroks in Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.     Consider a matrix $\\textbf{M}$ of size $(n, n)$. Flatten this into a 1-dimensional array and \n",
    "> 1. compute **mean** and **standard deviation** of $\\textbf{M}$ in *two* ways. <br>\n",
    "> 2. Apply the element wise operation as defined below: $$z_i = \\frac{x_i - \\mu}{\\sigma}$$ <br> where $x_i, \\ \\mu,\\ \\sigma$ are elements, mean and standard deviation of flattened matrix $\\textbf{M}$ respectively. And $z$ is the output vector.\n",
    "> 3. Compute the **mean** and **standard deviation** of $z$ and compare them with the **mean** and **standard deviation** of $\\textbf{M}$.\n",
    "> 4. Resaon about the above comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "M = np.random.rand(n, n)\n",
    "M_flat = M.reshape(-1)\n",
    "print(M.shape)\n",
    "print(M_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part - 1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_flat_mean_1 = M_flat.mean()\n",
    "M_flat_mean_2 = M_flat.sum()/M_flat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_flat_std_1 = M_flat.std()\n",
    "M_flat_std_2 = np.sqrt(((M_flat - M_flat.mean())**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 1 :  0.5018681394383812\n",
      "Mean 2 :  0.5018681394383812\n",
      "StD 1 :  0.2891829388347725\n",
      "StD 2 :  0.2891829388347725\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean 1 : \", M_flat_mean_1)\n",
    "print(\"Mean 2 : \", M_flat_mean_2)\n",
    "print(\"StD 1 : \", M_flat_std_1)\n",
    "print(\"StD 2 : \", M_flat_std_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part - 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(M_flat):\n",
    "    mean = M_flat.mean()\n",
    "    std = M_flat.std()\n",
    "    return (M_flat - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fun(M_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part - 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean = z.mean()\n",
    "z_std = z.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_flat's mean :  0.5018681394383812 \t z's mean :  -1.0338396805309458e-16\n",
      "M_flat's std :  0.2891829388347725 \t z's std :  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"M_flat's mean : \",M_flat_mean_1, '\\t', \"z's mean : \",z_mean)\n",
    "print(\"M_flat's std : \", M_flat_std_1, '\\t', \"z's std : \", z_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part - 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  1. We can observe that there are multiple ways to compute **mean** and **standard deviation** and we can use any methods as per our need.\n",
    ">  2. The function `fun(M_flat)` actually calculates the Z-score of given data or normalizes the given data such that the **mean** and **standard deviation** are $0$ and $1$ respectively. And this is true for any data with any **mean** and **standard deviation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.    Consider an $n$ dimentional vector $\\vec{V}$ (having $n$ elements), calculate :\n",
    "> 1. $|\\vec{V}|$ (magnitude of vector)\n",
    "> 2. $\\sum_{i=1}^{n}v_i^3$ in three different ways (here $n$ is total number of elements in $\\vec{V}$ and $v_i$ is $i_{th}$ element of $\\vec{V}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "V = np.random.randn(n)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part - 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.058402897550943\n"
     ]
    }
   ],
   "source": [
    "V_magnitude = np.sqrt((V**2).sum())\n",
    "print(V_magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part - 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_cube_1 = np.sum(V**3)\n",
    "V_cube_2 = np.power(V, 3).sum()\n",
    "V_cube_3 = np.dot(V * V, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1:  -0.8532304329563993\n",
      "Method 2:  -0.8532304329563993\n",
      "Method 3:  -0.8532304329563976\n"
     ]
    }
   ],
   "source": [
    "print('Method 1: ', V_cube_1)\n",
    "print('Method 2: ', V_cube_2)\n",
    "print('Method 3: ', V_cube_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">  1. Here you can see that, we can compute the same expression with different methods depending upon the structure and requirement of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  Create two vectors $y$ and $\\hat{y}$ having **same** dimensions, where $\\hat{y}$ should consist of random numbers between $[0, 1]$ and $y$ should contain $0s$ and $1s$, for example $y = [0, 1, 1, 0, 1, 0, 0, 1, ..., 1]$. Compute the given expression: $$O = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\log_2(\\hat{y_i}) + (1-y_i)\\log_2(1-\\hat{y_i})]$$\n",
    "where $n$ is the total number of elements in $y$ and $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "y = np.random.randint(0, 2, 100)\n",
    "y_hat = np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
      " 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0\n",
      " 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.42751397e-02 5.10120087e-01 3.62371076e-01 1.36817130e-01\n",
      " 1.14958228e-01 8.16861715e-01 8.07895967e-02 5.01353723e-01\n",
      " 6.35573527e-01 6.12210935e-01 2.78992247e-01 5.17490783e-01\n",
      " 4.77171591e-01 2.15717133e-01 3.47617265e-01 6.09422686e-01\n",
      " 3.63749211e-01 1.41289045e-01 9.42658803e-01 4.86795004e-01\n",
      " 6.92951649e-01 5.41012140e-01 2.36484831e-01 5.79050489e-01\n",
      " 5.52569038e-01 1.57536013e-01 2.36294404e-01 4.42452443e-01\n",
      " 6.68629415e-01 6.72047916e-02 6.40302315e-01 3.34410859e-01\n",
      " 7.91112636e-01 5.10854150e-01 5.72216512e-01 1.97534310e-01\n",
      " 5.94188488e-01 2.58619675e-01 6.23423346e-01 3.53971741e-01\n",
      " 1.76675066e-01 1.88965145e-02 2.34338631e-02 9.51921835e-01\n",
      " 1.06909083e-01 6.23990809e-01 2.36047497e-01 3.97564606e-01\n",
      " 1.59732734e-01 1.85407896e-02 8.72069255e-01 6.42779642e-01\n",
      " 5.41749974e-01 3.27633832e-01 5.48352145e-01 7.40087652e-01\n",
      " 4.70933029e-01 9.76700797e-01 6.41281410e-01 8.98989163e-01\n",
      " 1.62859690e-01 4.32040931e-04 3.75298210e-01 5.30229966e-01\n",
      " 5.21145270e-01 9.74280962e-01 8.60527971e-01 4.06286026e-01\n",
      " 5.59314244e-01 3.71117450e-01 5.77317223e-01 6.38130834e-01\n",
      " 8.36183332e-01 2.29914954e-01 1.90300162e-01 5.42931238e-01\n",
      " 3.88476712e-01 7.04633478e-01 6.53025588e-01 9.82646655e-02\n",
      " 4.68858436e-01 7.38463807e-01 5.48627053e-01 7.59630072e-01\n",
      " 7.35491391e-01 9.51571581e-01 9.44912784e-01 1.59911086e-02\n",
      " 4.00184361e-02 6.90626222e-01 4.66805790e-02 9.93197702e-02\n",
      " 4.14388070e-01 3.60190601e-01 4.56007809e-01 3.80992026e-01\n",
      " 6.59185057e-01 9.92093461e-01 8.53342927e-01 6.30849269e-01]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(y, y_hat):\n",
    "    temp_sum = (y * np.log2(y_hat) + (1 - y) * np.log2(1 - y_hat))\n",
    "    return -temp_sum.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    "O = fun(y, y_hat)\n",
    "print(O) \n",
    "\n",
    ">  1. The expression $O = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\log_2(\\hat{y_i}) + (1-y_i)\\log_2(1-\\hat{y_i})]$, which you have computed is actually a **Cross-Entropy** loss function used in machine learning for classification task which tells us how bad or good model is performing, if $O$ is large then model is performing worst and vice versa.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
